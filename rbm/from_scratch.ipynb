{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot off the press from chatgpt o3-mini-high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import jit \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeRBM:\n",
    "    def __init__(self, n_hidden, data_path=None, key=None):\n",
    "        \"\"\"\n",
    "        Initializes the RBM with the given number of hidden units.\n",
    "        Data is expected to be arranged such that each column is a sample\n",
    "        and each row is a variable.\n",
    "        If no random key is provided, a default one is created.\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = random.PRNGKey(0)\n",
    "        self.key = key\n",
    "        self.data = self.load_data(data_path)\n",
    "        self.set_empirical_means()\n",
    "        self.n_visible = self.data.shape[0]  # each row is a variable\n",
    "        self.n_hidden = n_hidden\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        # Initialize weights and biases.\n",
    "        # W has shape (n_hidden, n_visible)\n",
    "        self.W = random.normal(subkey, shape=(n_hidden, self.n_visible)) * 0.1\n",
    "        self.h_bias = jnp.zeros((n_hidden,))\n",
    "        self.v_bias = jnp.zeros((self.n_visible,))\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        if data_path is None:\n",
    "            mnist = fetch_openml('mnist_784', version=1)\n",
    "            # Original shape is (n_samples, n_features). Transpose to have shape (n_features, n_samples).\n",
    "            X = np.array(mnist.data.T, dtype=np.float32) / 255.0  # Normalize to [0,1]\n",
    "            print(\"Data shape:\", X.shape)\n",
    "            X = (X > 0.5).astype(np.float32)  # Binarize the data\n",
    "        else:\n",
    "            # Assume CSV has samples as rows; transpose to get columns as samples.\n",
    "            X = pd.read_csv(data_path, index_col=0).values.T\n",
    "        # Split into training and test sets.\n",
    "        # train_test_split expects samples as rows, so we transpose before and after splitting.\n",
    "        X_train, X_test = train_test_split(X.T, test_size=0.2, random_state=int(self.key[0]))\n",
    "        # Transpose back so that each column is a sample.\n",
    "        X_train = jnp.array(X_train.T)\n",
    "        X_test = jnp.array(X_test.T)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        return X\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "    def sample_h(self, v):\n",
    "        \"\"\"\n",
    "        Given a batch of visible units (v) with shape (n_visible, batch_size),\n",
    "        sample the hidden units.\n",
    "        Returns both the binary samples and their probabilities.\n",
    "        \"\"\"\n",
    "        # Compute hidden activations: shape (n_hidden, batch_size)\n",
    "        p_h = self.sigmoid(jnp.dot(self.W, v) + self.h_bias[:, None])\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        h = random.bernoulli(subkey, p_h).astype(jnp.float32)\n",
    "        return h, p_h\n",
    "\n",
    "    def sample_v(self, h):\n",
    "        \"\"\"\n",
    "        Given a batch of hidden units (h) with shape (n_hidden, batch_size),\n",
    "        sample the visible units.\n",
    "        Returns both the binary samples and their probabilities.\n",
    "        \"\"\"\n",
    "        # Compute visible activations: shape (n_visible, batch_size)\n",
    "        p_v = self.sigmoid(jnp.dot(self.W.T, h) + self.v_bias[:, None])\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        v = random.bernoulli(subkey, p_v).astype(jnp.float32)\n",
    "        return v, p_v\n",
    "\n",
    "    def contrastive_divergence(self, v0, k=1):\n",
    "        \"\"\"\n",
    "        Performs k steps of Gibbs sampling starting at the visible units v0.\n",
    "        v0 is expected to have shape (n_visible, batch_size).\n",
    "        \"\"\"\n",
    "        vk = v0\n",
    "        for _ in range(k):\n",
    "            h, _ = self.sample_h(vk)\n",
    "            vk, _ = self.sample_v(h)\n",
    "        return vk\n",
    "\n",
    "    def train_batch(self, v0, learning_rate=0.01, k=1, l2_reg=0.0):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the RBM using Contrastive Divergence.\n",
    "        \n",
    "        Parameters:\n",
    "          v0 (array): Batch of visible units with shape (n_visible, batch_size).\n",
    "          learning_rate (float): Learning rate for the update.\n",
    "          k (int): Number of Gibbs sampling steps.\n",
    "          l2_reg (float): L2 regularization coefficient.\n",
    "        \"\"\"\n",
    "        vk = self.contrastive_divergence(v0, k)\n",
    "        h0, _ = self.sample_h(v0)\n",
    "        hk, _ = self.sample_h(vk)\n",
    "        batch_size = v0.shape[1]\n",
    "        # Compute gradients (note: outer products are computed by dotting with the transpose).\n",
    "        delta_W = jnp.dot(h0, v0.T) - jnp.dot(hk, vk.T)\n",
    "        delta_v_bias = jnp.sum(v0 - vk, axis=1)  # Sum over samples (columns)\n",
    "        delta_h_bias = jnp.sum(h0 - hk, axis=1)  # Sum over samples (columns)\n",
    "        # Update parameters with optional L2 regularization.\n",
    "        self.W = self.W + learning_rate * (delta_W / batch_size - l2_reg * self.W)\n",
    "        self.v_bias = self.v_bias + learning_rate * delta_v_bias / batch_size\n",
    "        self.h_bias = self.h_bias + learning_rate * delta_h_bias / batch_size\n",
    "\n",
    "    def reconstruct(self, v, k=1):\n",
    "        \"\"\"\n",
    "        Reconstructs the input visible units by running Gibbs sampling.\n",
    "        v is expected to have shape (n_visible, batch_size).\n",
    "        \"\"\"\n",
    "        return self.contrastive_divergence(v, k)\n",
    "\n",
    "    def generate(self, n_samples, gibbs_steps=100):\n",
    "        \"\"\"\n",
    "        Generates new samples from the model by starting with random noise and\n",
    "        running Gibbs sampling for a specified number of steps.\n",
    "        \n",
    "        Returns:\n",
    "          samples with shape (n_visible, n_samples), where each column is a sample.\n",
    "        \"\"\"\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        # Initialize samples with shape (n_visible, n_samples)\n",
    "        samples = random.bernoulli(subkey, p=jnp.ones((self.n_visible, n_samples)) * 0.5).astype(jnp.float32)\n",
    "        for _ in range(gibbs_steps):\n",
    "            h, _ = self.sample_h(samples)\n",
    "            samples, _ = self.sample_v(h)\n",
    "        return samples\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_averages(samples):\n",
    "        \"\"\"\n",
    "        Computes the mean (per variable) and covariance from the samples.\n",
    "        Samples are expected to have shape (n_visible, n_samples).\n",
    "        \"\"\"\n",
    "        return jnp.mean(samples, axis=1), jnp.cov(samples)\n",
    "\n",
    "    def set_empirical_means(self):\n",
    "        self.mean, self.cov = self.compute_averages(self.data)\n",
    "\n",
    "    def compute_squared_deviations(self, samples):\n",
    "        sample_mean, cov = self.compute_averages(samples)\n",
    "        return (sample_mean - self.mean)**2, (cov - self.cov)**2\n",
    "\n",
    "    def split_data_in_half(self):\n",
    "        cols = self.data.shape[1]\n",
    "        perm = jax.random.permutation(self.key, cols)\n",
    "        fh_indices = perm[:cols // 2]\n",
    "        sh_indices = perm[-cols // 2:]\n",
    "        first_half_data = self.data[:, fh_indices]\n",
    "        second_half_data = self.data[:, sh_indices]\n",
    "        return first_half_data, second_half_data\n",
    "\n",
    "    def background_model_squared_deviations(self):\n",
    "        first_half_data, second_half_data = self.split_data_in_half()\n",
    "        fhm, fhc = self.compute_averages(first_half_data)\n",
    "        shm, shc = self.compute_averages(second_half_data)\n",
    "        return (fhm - shm)**2, (fhc - shc)**2\n",
    "\n",
    "    def compute_background_rmse(self):\n",
    "        mean_diffs, cov_diffs = self.background_model_squared_deviations()\n",
    "        return jnp.sqrt(jnp.mean(mean_diffs)), jnp.sqrt(jnp.mean(cov_diffs.flatten()))\n",
    "\n",
    "    def compute_rmse(self, samples):\n",
    "        mean_devs, cov_devs = self.compute_squared_deviations(samples)\n",
    "        return jnp.sqrt(jnp.mean(mean_devs)), jnp.sqrt(jnp.mean(cov_devs.flatten()))\n",
    "\n",
    "    def fit(self, X_train, epochs=10, batch_size=64, learning_rate=0.01, k=1, l2_reg=0.0, sample_number=1000):\n",
    "        \"\"\"\n",
    "        Trains the RBM on the training data.\n",
    "        \n",
    "        Parameters:\n",
    "          X_train (array): Training data as a JAX array with shape (n_visible, n_train),\n",
    "                           where each column is a sample.\n",
    "          epochs (int): Number of epochs.\n",
    "          batch_size (int): Number of samples per batch.\n",
    "          learning_rate (float): Learning rate.\n",
    "          k (int): Number of Gibbs sampling steps per batch.\n",
    "          l2_reg (float): L2 regularization coefficient.\n",
    "          \n",
    "        Returns:\n",
    "          losses: A list of reconstruction losses per epoch.\n",
    "          sample_list: An array containing generated samples at intervals.\n",
    "        \"\"\"\n",
    "        num_samples = X_train.shape[1]\n",
    "        losses = []\n",
    "        sample_list = []\n",
    "        # Shuffle training data along the sample axis (columns).\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_train = X_train[:, permutation]\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                batch = X_train[:, i:i+batch_size]\n",
    "                self.train_batch(batch, learning_rate=learning_rate, k=k, l2_reg=l2_reg)\n",
    "                v_recon = self.reconstruct(batch)\n",
    "                epoch_loss += jnp.sum((batch - v_recon) ** 2)\n",
    "            losses.append(epoch_loss)\n",
    "            if (epoch * 10) % epochs == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}, Reconstruction Loss: {epoch_loss:.4f}\")\n",
    "                samples = self.generate(sample_number, gibbs_steps=1000)\n",
    "                sample_list.append(samples)\n",
    "        return losses, jnp.array(sample_list)\n",
    "\n",
    "    def plot_deviations_over_time(self, train_args):\n",
    "        _, samples = self.fit(**train_args)\n",
    "        fig, ax = plt.subplots()\n",
    "        errors = jnp.array(jax.vmap(self.compute_rmse)(samples)) \n",
    "        background_mean, background_cov = self.compute_background_rmse()\n",
    "        ax.plot(errors[:, 0]) \n",
    "        ax.plot(errors[:, 1]) \n",
    "        ax.hlines(background_mean, 0, len(errors))\n",
    "        ax.hlines(background_cov, 0, len(errors)) \n",
    "        return fig, ax, errors \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# Demonstration code:\n",
    "def demo_generative_rbm(epochs=10):\n",
    "\n",
    "    # Create the RBM instance.\n",
    "    rbm = GenerativeRBM(n_hidden=128)\n",
    "\n",
    "    # Train the RBM.\n",
    "    losses, samples = rbm.fit(rbm.X_train, epochs=epochs, batch_size=64, learning_rate=0.01, k=1, l2_reg=0.001)\n",
    "    return rbm, losses, samples \n",
    "    # Reconstruct test samples.\n",
    "    test_samples = rbm.X_test[:, :10]\n",
    "    v_recon = rbm.reconstruct(test_samples)\n",
    "\n",
    "    # Plot original and reconstructed images.\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(10, 2))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(np.array(test_samples[:, i]).reshape(28, 28), cmap=\"gray\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(np.array(v_recon[:, i]).reshape(28, 28), cmap=\"gray\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "    plt.suptitle(\"Top: Original | Bottom: Reconstruction\")\n",
    "    plt.show()\n",
    "\n",
    "    # Generate new samples.\n",
    "    generated_samples = rbm.generate(n_samples=10, gibbs_steps=100)\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(10, 1))\n",
    "    for i in range(10):\n",
    "        axes[i].imshow(np.array(generated_samples[:, i]).reshape(28, 28), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.suptitle(\"Newly Generated Samples\")\n",
    "    plt.show()\n",
    "    return rbm, losses, samples \n",
    "\n",
    "# To run the demonstration, simply call:\n",
    "# demo_generative_rbm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (784, 70000)\n",
      "Epoch 0/10, Reconstruction Loss: 6668134.0000\n",
      "Epoch 1/10, Reconstruction Loss: 4764933.0000\n",
      "Epoch 2/10, Reconstruction Loss: 4311073.0000\n",
      "Epoch 3/10, Reconstruction Loss: 4037872.0000\n",
      "Epoch 4/10, Reconstruction Loss: 3850002.0000\n",
      "Epoch 5/10, Reconstruction Loss: 3707276.0000\n",
      "Epoch 6/10, Reconstruction Loss: 3593191.0000\n",
      "Epoch 7/10, Reconstruction Loss: 3500305.0000\n",
      "Epoch 8/10, Reconstruction Loss: 3423463.0000\n",
      "Epoch 9/10, Reconstruction Loss: 3355048.0000\n"
     ]
    }
   ],
   "source": [
    "P = 2\n",
    "rbm, losses, samples = demo_generative_rbm(epochs=P) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (784, 70000)\n"
     ]
    }
   ],
   "source": [
    "rbm = GenerativeRBM(n_hidden=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {'X_train': rbm.X_train, \n",
    "              'epochs': 3,\n",
    "              'batch_size': 64, \n",
    "              'learning_rate': 0.01, \n",
    "              'k': 1, \n",
    "              'l2_reg': 0.0, \n",
    "              'sample_number': 1000}\n",
    "\n",
    "rbm.plot_deviations_over_time(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 784, 1000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = jnp.array(jax.vmap(rbm.compute_rmse)(samples)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for 10 epochs buys a 50% decrease in reconstruction error, but the newly generated samples look like white noise. 1000 epochs is much better--the new samples look great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olfaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
