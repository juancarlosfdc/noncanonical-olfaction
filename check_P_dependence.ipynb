{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canonical_model_jax import * \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(u): \n",
    "    return 1 / (1 + jnp.exp(-u))\n",
    "\n",
    "def psi(x): \n",
    "    return jnp.log(x / (1 - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ps = jnp.linspace(1000, 50000, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OlfactorySensing:\n",
    "    def __init__(self, N=100, n=2, M=30, P=1000, sigma_0=1e-2, sigma_c=2.0): \n",
    "        self.N = N\n",
    "        self.n = n\n",
    "        self.M = M\n",
    "        self.P = P\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.sigma_c = sigma_c\n",
    "        self.set_sigma()\n",
    "        self.set_vasicek_window()\n",
    "        self.W = None  # Initialize W as None; it may be set later with set_random_W\n",
    "\n",
    "    def _tree_flatten(self):\n",
    "        # Treat `W` as a dynamic value, while the rest are static\n",
    "        children = (self.W,)  # W is the only dynamic value\n",
    "        aux_data = {\n",
    "            'N': self.N,\n",
    "            'n': self.n,\n",
    "            'M': self.M,\n",
    "            'P': self.P,\n",
    "            'sigma_0': self.sigma_0,\n",
    "            'sigma_c': self.sigma_c,\n",
    "            'vasicek_window': self.vasicek_window,\n",
    "        }\n",
    "        return (children, aux_data)\n",
    "\n",
    "    @classmethod\n",
    "    def _tree_unflatten(cls, aux_data, children):\n",
    "        # Recreate an instance of OlfactorySensing from children and aux_data\n",
    "        instance = cls(\n",
    "            N=aux_data['N'],\n",
    "            n=aux_data['n'],\n",
    "            M=aux_data['M'],\n",
    "            P=aux_data['P'],\n",
    "            sigma_0=aux_data['sigma_0'],\n",
    "            sigma_c=aux_data['sigma_c']\n",
    "        )\n",
    "        instance.W = children[0]\n",
    "        instance.vasicek_window = aux_data['vasicek_window']\n",
    "        return instance\n",
    "\n",
    "    def set_sigma(self): \n",
    "        self.sigma = lambda x: x / (1 + x) \n",
    "\n",
    "    def set_vasicek_window(self): \n",
    "        self.vasicek_window = jax.lax.stop_gradient(jnp.round(jnp.sqrt(self.P) + 0.5)).astype(int)\n",
    "    \n",
    "    @jit \n",
    "    def draw_cs(self, key):\n",
    "        # Split the key into subkeys for indices and concentrations\n",
    "        subkeys = jax.random.split(key, self.P + 1)\n",
    "        indices_key = subkeys[0]\n",
    "        concentration_keys = subkeys[1:]\n",
    "\n",
    "        # Generate indices for all samples (P x n)\n",
    "        indices = jax.vmap(\n",
    "            lambda k: jax.random.choice(k, self.N, shape=(self.n,), replace=False)\n",
    "        )(jax.random.split(indices_key, self.P))\n",
    "\n",
    "        # Generate concentrations for all samples (P x n)\n",
    "        concentrations = jax.vmap(\n",
    "            lambda k: jax.random.lognormal(k, sigma=self.sigma_c, shape=(self.n,))\n",
    "        )(concentration_keys)\n",
    "\n",
    "        # Initialize the full samples matrix (P x N)\n",
    "        c = jnp.zeros((self.P, self.N))\n",
    "\n",
    "        # Scatter the concentrations into the appropriate indices\n",
    "        c = c.at[jnp.arange(self.P)[:, None], indices].set(concentrations)\n",
    "\n",
    "        # Return the result transposed\n",
    "        return c.T\n",
    "\n",
    "\n",
    "    def set_random_W(self, key): \n",
    "        self.W = 1 / jnp.sqrt(self.N) * jax.random.normal(key, shape=(self.M, self.N))\n",
    "\n",
    "    def compute_activity(self, W, c, key): \n",
    "        pre_activations = W @ c\n",
    "        r = self.sigma(pre_activations) + self.sigma_0 * jax.random.normal(key, shape=pre_activations.shape) \n",
    "        return r\n",
    "\n",
    "    # @jit # This might take hours to compile.  https://jax.readthedocs.io/en/latest/control-flow.html#control-flow is helpful for the following\n",
    "    def compute_entropy_of_r(self, W, c, key):\n",
    "        r = self.compute_activity(W, c, key)\n",
    "        entropy = self.compute_sum_of_marginal_entropies(r) - self.compute_information_of_r(r)\n",
    "        return entropy\n",
    "\n",
    "    @jit \n",
    "    def compute_sum_of_marginal_entropies(self, r):\n",
    "        compute_entropy_vmap = vmap(self._vasicek_entropy, in_axes=0)\n",
    "        # Apply the vectorized function\n",
    "        marginal_entropies = compute_entropy_vmap(r)\n",
    "        # Sum the marginal entropies\n",
    "        return jnp.sum(marginal_entropies)\n",
    "\n",
    "    @jit \n",
    "    def compute_information_of_r(self, r): \n",
    "        M, P = r.shape\n",
    "        G = norm.ppf((rankdata(r.T, axis=0) / (P + 1)), loc=0, scale=1) # this is just ranking the data and making it normally distributed. \n",
    "        bias_correction = 0.5 * jnp.sum(digamma((P - jnp.arange(1, M + 1) + 1) / 2) - jnp.log(P / 2)) \n",
    "        cov_matrix = jnp.cov(G, rowvar=False)\n",
    "        chol_decomp = cholesky(cov_matrix)\n",
    "        log_det = jnp.sum(jnp.log(jnp.diag(chol_decomp)))\n",
    "        I = -(log_det - bias_correction) # remember: entropy overall is sum of marginals minus information. information is sum of marginals - entropy. Kind of stupid. \n",
    "        return I\n",
    "    \n",
    "    def sum_covariances(self, W, c, key): \n",
    "        r = self.compute_activity(W, c, key)\n",
    "        cov_r = jnp.cov(r) \n",
    "        off_diag_mask = ~jnp.eye(cov_r.shape[0], dtype=bool)\n",
    "        # Extract the off-diagonal elements\n",
    "        off_diag_elements = cov_r[off_diag_mask]\n",
    "        # Compute the Frobenius norm of the off-diagonal elements\n",
    "        frob_norm = jnp.sqrt(jnp.sum(off_diag_elements**2))\n",
    "        return frob_norm \n",
    "    \n",
    "    def log_det_sigma(self, W, c, key):\n",
    "        r = self.compute_activity(W, c, key)\n",
    "        cov_r = jnp.cov(r)  \n",
    "        chol = cholesky(cov_r) \n",
    "        log_det = jnp.sum(jnp.log(jnp.diag(chol)))\n",
    "        return log_det\n",
    "\n",
    "    \n",
    "    def _pad_along_last_axis(self, X):\n",
    "        first_value = X[0]\n",
    "        last_value = X[-1]\n",
    "        # Use `lax.full_like` to create padded arrays\n",
    "        Xl = lax.full_like(x=jnp.empty((self.vasicek_window,)), fill_value=first_value)\n",
    "        Xr = lax.full_like(x=jnp.empty((self.vasicek_window,)), fill_value=last_value)\n",
    "        return jnp.concatenate((Xl, X, Xr))\n",
    "\n",
    "    def _vasicek_entropy(self, X):\n",
    "        n = X.shape[-1]\n",
    "        X = jnp.sort(X, axis=-1)\n",
    "        X = self._pad_along_last_axis(X)\n",
    "        start1 = 2 * self.vasicek_window\n",
    "        length = self.P\n",
    "        differences = lax.dynamic_slice(X, (start1,), (length, )) - lax.dynamic_slice(X, (0,), (length,))\n",
    "        logs = jnp.log(n / (2 * self.vasicek_window) * differences)\n",
    "        return jnp.mean(logs, axis=-1)\n",
    "\n",
    "# Register the custom class as a PyTree with JAX\n",
    "tree_util.register_pytree_node(\n",
    "    OlfactorySensing,\n",
    "    OlfactorySensing._tree_flatten,\n",
    "    OlfactorySensing._tree_unflatten\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, n, M, sigma_c, P = 60, 2, 30, 2.0, 50000\n",
    "os = OlfactorySensing(N=N, n=n, M=M, P=P, sigma_c=sigma_c)\n",
    "key = jax.random.PRNGKey(1) \n",
    "os.cs = os.draw_cs(key=key) \n",
    "W_init = jnp.clip(1 / jnp.sqrt(os.N) * jax.random.gamma(key, a=1, shape=(M, N)), min=0, max=1-(1e-10)) \n",
    "os.W = W_init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 58.095767974853516\n",
      "Step 2, Loss: 50.326011657714844\n",
      "Step 4, Loss: 47.06768798828125\n",
      "Step 6, Loss: 45.30582046508789\n",
      "Step 8, Loss: 44.08979797363281\n",
      "Step 10, Loss: 43.124271392822266\n",
      "Step 12, Loss: 42.208187103271484\n",
      "Step 14, Loss: 41.62626647949219\n",
      "Step 16, Loss: 41.119747161865234\n",
      "Step 18, Loss: 40.66557693481445\n"
     ]
    }
   ],
   "source": [
    "Ws, ents, losses = natural_gradient_dual_space(20, W_init, os.cs, key, lambda * args: - os.log_det_sigma(*args), 1, os, phi, psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olfaction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
